# Synthetic-Speech-Using-Text-to-Speech

![Cloud Text](https://github.com/EslamFouadd/Synthetic-Speech-Using-Text-to-Speech/assets/77150715/ac08d595-7f18-4ca9-add8-2c4a567fa00f)


## In this lab you use the Text-to-Speech API to do the following:

1. Create a series of audio files

2. Listen and compare audio files

3. Configure audio output


Task 1. Enable the Text-to-Speech API

Task 2. Create a virtual environment

Task 3. Create a service account

Task 4. Get a list of available voices

Task 5. Create synthetic speech from text

Task 6. Create synthetic speech from SSML

Task 7. Configure audio output and device profiles

## Understanding the results
In the results for your video annotation, Vertex AI provides three types of information:

Labels for the video: This information is on the Segment tab below the video on the results page.

Labels for shots within the video: This information is on the Shot tab below the video on the results page.

Labels for each 1-second interval within the video: This information is on the Interval tab below the video on the results page.

If the prediction fails, the results in the list show a red icon on the Recent Predictions list.

If only one video in the prediction attempt failed, the results show a green icon in the Recent Predictions list. On the results page for that prediction, you can view the results for the videos that Vertex AI has annotated.
